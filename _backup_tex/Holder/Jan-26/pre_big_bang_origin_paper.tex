\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

% --- Notation ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rp}{\R_{>0}}
\newcommand{\Rnn}{\R_{\ge 0}}
\newcommand{\J}{J}
\newcommand{\Rhat}{\widehat{\mathsf{R}}}
\newcommand{\phig}{\varphi}

% --- Theorem environments (used sparingly; most claims are stated as prose) ---
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{\textbf{The Pre--Big Bang Origin of Law:}\\
A Cost-First, Recognition-Minimizing Framework for Discreteness, Geometry, and Emergent Dynamics}
\author{Jonathan Washburn\\
\small (with a mechanized verification corpus in Lean)}
\date{January 2026}

\begin{document}
\maketitle

\begin{abstract}
The phrase ``before the Big Bang'' is usually treated as either a poetic question or a category error: in general relativity, ``before'' presupposes time, but classical time is defined by a spacetime geometry that itself breaks down at the Big Bang singular boundary. In this paper we propose a different, operational notion of origin: the \emph{pre-geometric selection regime} in which the existence of stable distinctions, reproducible regularities, and law-like dynamics is forced by a unique minimization principle, prior to any assumption of manifold structure, energy, or Hamiltonian time evolution.

Our starting point is a cost functional \(\J:\Rp\to\Rnn\) interpreted as the irreducible penalty of \emph{comparison} (deviation from balance). Under minimal structural requirements---reciprocal symmetry \(\J(x)=\J(1/x)\), normalization \(\J(1)=0\), and compositional consistency of multiplicative comparison---together with standard regularity and calibration, the functional form of \(\J\) is uniquely fixed (up to units) to
\[
\J(x)=\frac{1}{2}\bigl(x+x^{-1}\bigr)-1.
\]
From this cost landscape we derive a forcing chain: discreteness is required for stability; reciprocal symmetry enforces a double-entry ``ledger'' structure; self-similarity forces the golden ratio \(\phig\); and a minimal closed recognition cadence yields an 8-step cycle with an induced 8-point Fourier backbone. The fundamental dynamical object is not an energy Hamiltonian but a discrete evolution operator \(\Rhat\) that advances states by minimizing cumulative recognition cost subject to admissibility and conservation constraints; standard Hamiltonian dynamics arises only as a near-equilibrium approximation in the small-deviation regime.

Crucially, the framework is accompanied by an explicit claim ledger: each major statement is tagged as theorem, hypothesis-gated bridge, or modeling choice, and many core results are mechanized in a Lean verification corpus (the ``Indisputable Monolith''). This paper presents the scientific narrative and the falsifiable bridge program: what is proved today, what is assumed, where the seams are, and what measurements would refute the theory.
\end{abstract}

\section{Introduction}
\subsection{Why ``before the Big Bang'' is usually not a physics question}
The standard cosmological story begins with an extrapolation: run Einstein's equations backwards in time and the universe becomes denser and hotter until the classical description ceases to be trustworthy. In many models the failure is formalized by singularity theorems; in others it is softened by inflationary dynamics or replaced by a bounce, a tunneling event, or a no-boundary proposal. Yet almost all such approaches share a common feature: they start by assuming a spacetime arena (or at least a well-defined time parameter) and then ask what field configurations or quantum states inhabit that arena.

The phrase ``what happened \emph{before} the Big Bang'' smuggles in precisely what is missing at the boundary: a background time with respect to which ``before'' is meaningful. If time is emergent from a deeper structure, then the origin question should not be phrased as ``what was the earlier state?'' but as ``what forces a \emph{stable notion of state}, a \emph{stable notion of distinction}, and therefore a \emph{stable notion of law}?'' This is the question we call the \emph{pre--Big Bang origin}.

\subsection{Reframing origin as pre-geometric selection}
We define the origin problem operationally: identify the minimal assumptions under which a world can support (i) stable distinctions (so that ``this'' is not ``that''), (ii) reproducible regularities (so that prediction is meaningful), and (iii) a compressible description (so that laws exist as finite objects rather than as an unstructured lookup table). In this framing, an ``origin'' is not a moment in time but a \emph{selection principle} that chooses, from a vast space of possible configurations, a narrow basin of stable, reproducible structure.

Recognition Science (RS) adopts a cost-first foundation. Instead of taking energy, action, or probability as primitive, we take \emph{comparison cost} as primitive: any attempt to distinguish or relate configurations incurs an irreducible penalty unless the relation is perfectly balanced. The pre-geometric regime is then the regime in which the structure of that cost function, together with its stability requirements, forces the emergence of discreteness, conservation, scaling hierarchy, and an effective geometry.

\subsection{The canonical comparison cost}
Let \(\J:\Rp\to\Rnn\) be a cost assigned to a positive ratio \(x\) (``how far'' one configuration is from another, in multiplicative terms). Three requirements are treated as structural features of comparison:
\begin{enumerate}
  \item \textbf{Reciprocal symmetry}: \(\J(x)=\J(1/x)\). Comparing \(x\) to unity is the same as comparing unity to \(x\).
  \item \textbf{Normalization}: \(\J(1)=0\). No deviation means no cost.
  \item \textbf{Compositional consistency}: the cost of a composed comparison is a fixed function of the component costs (a multiplicative consistency constraint).
\end{enumerate}
Under regularity conditions (smoothness) and a calibration convention fixing the local curvature scale at the minimum (e.g.\ \(\J''(1)=1\)), these requirements uniquely determine the functional form
\begin{equation}
\label{eq:Jcost}
\J(x)=\frac{1}{2}\bigl(x+x^{-1}\bigr)-1.
\end{equation}
Equation~\eqref{eq:Jcost} is not introduced as a model ansatz in this paper; it is the unique fixed point of the comparison requirements above (up to rescaling), with a mechanized proof path in the accompanying Lean corpus.

\subsection{From cost to world: overview of the forcing chain}
Given a canonical cost landscape, ``existence'' becomes a stability criterion: configurations that can realize zero defect (zero cost) are admissible minima; configurations that require divergent cost are excluded. From there the RS forcing chain proceeds, schematically:
\begin{itemize}
  \item \textbf{Discreteness}: stable minimizers under \(\J\) cannot persist in a fully continuous drift regime; stable posting occurs in discrete ticks.
  \item \textbf{Ledger structure}: reciprocal symmetry enforces a double-entry accounting of recognition events, yielding conservation constraints as the cancellation of reciprocal imbalances.
  \item \textbf{Self-similarity and scale}: discrete ledger stability forces a unique scaling ratio, identified with the golden ratio \(\phig\), and thereby a \(\phig\)-lattice of stable intensity levels.
  \item \textbf{Eight-tick cadence}: the minimal closed recognition loop supporting neutral (mean-free) structure yields an 8-step cycle; the 8-point discrete Fourier transform becomes the canonical diagonal basis for tick-translation.
  \item \textbf{Dynamics}: the fundamental evolution object is a recognition operator \(\Rhat\) advancing states to minimize cumulative recognition cost subject to admissibility and conservation. Traditional Hamiltonian dynamics appears only as an emergent approximation near equilibrium (small deviations from \(x=1\)).
\end{itemize}

\subsection{Claim hygiene: what is proved, what is assumed, what is modeled}
Because origin claims are easy to overstate, we adopt a strict discipline throughout: every major statement is tagged as one of (i) theorem (mechanized or fully proved), (ii) hypothesis-gated bridge (the intended derivation is explicit but currently relies on axioms or placeholders), or (iii) modeling choice (a convenient representation not claimed inevitable). We also provide a ``claim ledger'' mapping each paper-level claim to its precise formal anchor in the Lean corpus and, where relevant, to an audit certificate.

\subsection{Roadmap}
The remainder of the paper proceeds from foundations to consequences. Section~2 formalizes the question-theoretic notion of origin (well-formed, forced, gapped, or dissolved questions). Section~3 develops the inevitability of the canonical cost and the four-gate proof spine. Sections~4--6 present the forcing chain from cost to discreteness, ledger conservation, scaling, and the eight-tick backbone, culminating in the recognition operator \(\Rhat\). Section~7 derives the emergent Hamiltonian limit and delineates regimes where Hamiltonian reasoning should fail. Sections~8--10 discuss the geometry/metric bridge program and the cosmological interpretation of the ``bang'' as a regime transition. Later sections extend the framework to semantic atoms (WTokens) and, optionally, to consciousness and ethics, with preregistered empirical tests and explicit falsifiers.

\end{document}



